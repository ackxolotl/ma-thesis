\chapter{Introduction}
\label{chap:introduction}

High-speed network environments have seen a major shift towards virtualization
and cloud computing in the last two decades. This development began in the early
2000s, when Internet giants like Amazon, Google and Facebook had to cope with
rapid company growth and a sharp expansion of their infrastructure. By deploying
virtualization techniques on their servers, these companies managed to create
scalable platforms for their services while utilizing existing hardware
resources more efficiently.

Virtualization in this context does not necessarily refer to hardware
virtualization or virtual machines. Various solutions were used by the big
companies to abstract and unify their computing environments, from virtual
machines to custom software for storage/network virtualization or \ac{os}-level
virtualization like containers later on. Google, for example, is known to have
dispensed with classic virtualization in the form of hypervisor and virtual
machines on its platforms early on. For its cluster management software Borg,
Google preferred custom solutions and eventually containerization due to the
relatively high costs of classic virtualization \cite{verma2015large}.

Telecommunication providers have also begun to use virtualization techniques on
a broader basis. Through \ac{sdn}, providers such as Orange, Verizon and
Deutsche Telekom \cite{csikor2020transition} have created more flexible,
centrally managed networks. Google uses \ac{sdn} on a large scale, e.g., for its
B4 datacenter WAN \cite{jain2013b4}. Another virtualization technique employed
by telecommunication providers and IT companies is \ac{nfv}, i.e.,
virtualization of functionality traditionally implemented in dedicated hardware
like NAT, load balancers or firewalls \cite{roy2018state}. \ac{sdn} and \ac{nfv}
offer several benefits to these enterprises. With \ac{sdn}, networks can be
created and orchestrated remotely, services can be chained on-the-fly, and
providers can mitigate traffic bursts by dynamically adapting routes and
bandwidths. With \ac{nfv}, dedicated middle-boxes can be reduced and replaced by
off-the-shelf hardware, and manual work is no longer needed to adapt network
functionality.

A side effect of large-scale deployment of virtualization techniques by the
large IT companies was the invention of cloud computing. Like virtualization,
cloud computing is primarily a cost-saving measure: Companies rent out their
scalable platforms to other companies and end users to maximize utilization of
their infrastructure. Major companies use cloud computing as part of their
business models, namely Amazon with AWS since 2006, Microsoft with Microsoft
Azure since 2010 and Google with the Google Compute Engine since 2012.

Cloud computing offers a wide range of possible applications and is an active
subject of research by companies and universities. Topics of interests are, for
example, secure computing in the cloud, large scale data analysis or load
balancing.

While not necessarily being built on classic virtualization technique
themselves, Amazon AWS, Microsoft Azure and Google Compute Engine offer virtual
machines to customers. For high-performance applications, features like Amazon's
``Enhanced Networking'' are available which mitigate one of the limiting factors
in virtualized environments: poor performance of \acs{io} devices. In order to
achieve high throughput rates and low latency, direct pass-through of devices to
virtualized environments is essential. However, as hardware in current computer
systems is generally considered trustworthy and \ac{io} devices have unrestriced
access to host memory by design, devices cannot be passed-through in a safe
manner as they are since software in virtual environments could take over its
host by abusing the device's memory access privileges.

The fact that \ac{io} devices have unrestricted access to memory is a
consequence of historic developments. In the earliest computers, data transfer
between memory, \acs{cpu} and peripherals was done by the \ac{cpu}. However,
\acp{cpu} are not particularly good at copying data, and \ac{io} operations are
expensive. To reduce the time spent on memory transactions by the CPU, \ac{dma}
was introduced. With \ac{dma}, data transfers between \ac{io} devices and memory
were initially performed by a separate controller (third-party \ac{dma}) and
later by the \ac{io} devices themselves (first-party \ac{dma}). As the term
implies, devices that use \ac{dma} address memory directly, hence have
unrestricted access to host memory. This technique is still employed by \ac{io}
devices today, first and foremost by \acs{pcie} devices.

For fast pass-through of \ac{io} devices with real hardware isolation, computer
manufacturers included a new component in their systems called \acf{iommu}.
Similar to \acp{mmu}, \acp{iommu} introduced virtual memory for \ac{io} devices
and a mechanism to restrict \ac{dma} accesses of peripherals to the virtualized
memory. With such an \ac{iommu} that can only be configured by the hypervisor
or the operating system of the host, pass-through can be performed in a safe
manner.

In fact, \acp{iommu} or their predecessors were used before the advent of
\ac{io} virtualization technology on x86 in 2008. Some of the first machines
with an \ac{iommu} were the SPARC stations of the Sun-4 architecture launched in
1987. As early bus protocols at times supported smaller address spaces than the
hosts they were used in, it is suspected that these early \acp{iommu} were
included into the SPARC computers to improve \ac{io} performance by avoiding
bounce buffers, i.e., expensive copying of memory between \ac{dma}-able memory
and buffers in non-\ac{dma}-able memory
\cite[pp.~28~ff.]{rothwell2018exploitation}. Another early \ac{iommu} was the
Graphics Address Remapping Table (GART), used for graphic cards to provide a
contiguous view of host memory.

In 2005/2006, Intel (VT-x) and AMD (AMD-V) released new processor extensions to
support hardware virtualization. Slow \ac{io} performance of early Intel VT-x
processors \cite{adams2006comparison} was remedied by the first \acp{cpu}
employing Intel VT-d in 2008, i.e., Intel's \ac{io} virtualization features
including an \ac{iommu}, respectively with AMD-V in 2011 by AMD's FX \acp{cpu}.
Although primarily intended for virtualization, these \acp{iommu} were created
as multipurpose devices.

Usage of \acp{iommu} has changed. In recent years, protection against malicious
or faulty \ac{io} devices moves to the forefront. Necessity for protection in
this domain was already apparent in the early 2000s, when attacks through
externally connected \ac{dma}-able \ac{io} devices, e.g., using Apple's
FireWire, were carried out \cite{becher2005firewire}. With the mass adoption of
USB-C and Thunderbolt (another \ac{dma}-capable protocol), the topic is gaining
new importance.

Nowadays, \acp{iommu} are not only used in servers and desktop computers but
also mobiles devices like smartphones. Some ARM based Android phones are shipped
with \acp{iommu}, and Apple has developed its own variant for its iPhones to
protect the WiFi stack \cite{beniamini2017overtheair}.

In summary, we note that \acp{iommu} today are used for various purposes, from
virtualization in server architectures to protection against malicious
peripherals in consumer hardware. Due to their versatiliy and prevalence,
\acp{iommu} represent an interesting object of research. In this thesis, we
investigate performance and safety/security impacts of \acp{iommu} in high-speed
network environments with a focus on Intel's and AMD's \acp{iommu} on x86.

